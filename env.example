# Rename this file to `.env.example` (or `.env`) in your local checkout.
# Note: this project does NOT auto-load `.env` (no extra deps).
# Use one of:
#   - export $(cat .env | xargs) && python3 main.py
#   - direnv (recommended)
#   - your IDE run configuration env vars

# ----------------------------
# Vertex AI Gemini via LangChain (Vertex AI, no API keys)
# ----------------------------
# Auth prerequisite (one-time per machine/user):
#   gcloud auth application-default login
#
# Your GCP project id (required)
VERTEX_PROJECT_ID=your-gcp-project-id

# Vertex region (common: us-central1)
VERTEX_LOCATION=us-central1

# Default Gemini model name on Vertex (used as fallback)
VERTEX_MODEL=gemini-2.5-flash

# ----------------------------
# Agent/model settings (optional)
# ----------------------------
# Optional: override the Vertex model used by the reasoner (defaults to VERTEX_MODEL)
REASONER_MODEL_NAME=gemini-2.5-flash
REASONER_TEMPERATURE=0.7
# Leave empty to use provider/model default (no explicit max).
REASONER_MAX_TOKENS=

# Optional: override the Vertex model used by the judge (defaults to VERTEX_MODEL)
JUDGE_MODEL_NAME=gemini-2.5-pro
JUDGE_TEMPERATURE=0.0
# Leave empty to use provider/model default (no explicit max).
JUDGE_MAX_TOKENS=

# "select_one" or "synthesize_one"
SELECTION_STRATEGY=select_one

# if true, judge can synthesize a new tool call (subject to agent enforcement)
ALLOW_TOOL_SYNTHESIS=true

# Tracing/logging
TRACE=true
TOOL_RESULT_MAX_CHARS=400

# Timeout for waiting on K parallel reasoner calls (Vertex can be slower than a few seconds)
LLM_TIMEOUT_SECONDS=30.0


